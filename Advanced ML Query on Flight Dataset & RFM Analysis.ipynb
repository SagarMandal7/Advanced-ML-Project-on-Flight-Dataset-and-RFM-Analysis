{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Advanced Exam\n",
    "\n",
    "\n",
    "Perform the following tasks:\n",
    "\n",
    "### Flight Dataset\n",
    "\n",
    "**1. Perform Feature Engineering**  \n",
    "a) Perform basic exploration like checking for top 5 records, shape, statistical info, duplicates, Null values etc.  \n",
    "b) Extract Date, Month, Year from Date of Journey column  \n",
    "\n",
    "\n",
    "**2. Perform Exploratory Data Analysis (EDA) tasks**  \n",
    "a) Which airline is most preferred airline  \n",
    "b) Find the majority of the flights take off from which source  \n",
    "c) Find maximum flights land in which destination  \n",
    "\n",
    "\n",
    "**3. Compare independent features with Target feature to check the impact on price**  \n",
    "a) Which airline has the highest price  \n",
    "b) Check if the business class flights are high price or low and find only those flights which price is higher than 50k  \n",
    "\n",
    "\n",
    "**4. Perform encoding for the required features according to the data**  \n",
    "\n",
    "\n",
    "**5. Build multiple model by using different algorithm such as Linear Regression, Decision Tree, and Random Forest etc. and check the performance of your model.**  \n",
    "\n",
    "\n",
    "**6. Compare all of the models and justify your choice about the optimum model by using different evaluation technique and tune the models as per the requirement.**  \n",
    "\n",
    "\n",
    "**7. Write a conclusion from the business point of view. Finally perform the same preprocessing technique for test data best practice using pipeline.**  \n",
    "\n",
    "### RFM Dataset\n",
    "  \n",
    "**8. Calculate the**  \n",
    "\n",
    "a) recency (R),  \n",
    "b) frequency (F),  \n",
    "c) monetary value (M)  \n",
    "for each customer based on the given dataset?\n",
    "\n",
    "**9. Calculate RFM scores.**  \n",
    "a) Each customer will get a note between 1 and 5 for each parameter for Recency(R), Frequency(F) and Monetary value(M)  \n",
    "Ex: Scale for Recency:\n",
    "\n",
    "1) 0-30 days  \n",
    "2) 31-60 days  \n",
    "3) 61-90 days  \n",
    "4) 91-180 days  \n",
    "5) 181-365 days  \n",
    "\n",
    "b) Segment the customers based on their RFM scores using the dataset?\n",
    "Segments with RFM score range:  \n",
    "\n",
    "<body>\n",
    "\t<table>\n",
    "\t\t<thead>\n",
    "\t\t\t<tr>\n",
    "                <th><u>Segment</u></th>\n",
    "                <th><u>Description</u></th>\n",
    "\t\t\t</tr>\n",
    "\t\t</thead>\n",
    "\t\t<tbody>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Champions</td>\n",
    "\t\t\t\t<td>Bought recently, buy often and spend the most</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Loyal Customer</td>\n",
    "\t\t\t\t<td>Buy on regular basis. Responsive to Promotions.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Potential Loyalist</td>\n",
    "\t\t\t\t<td>Recent customer with average frequency.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Recent Customers</td>\n",
    "\t\t\t\t<td>Bought most recently, but not often.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Promising</td>\n",
    "\t\t\t\t<td>Recent shoppers, but have't spent much.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Customers Needing Attention</td>\n",
    "\t\t\t\t<td>Above average recency, frequency and monetary values. may not have bought very recently though.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>About to Sleep</td>\n",
    "\t\t\t\t<td>Below average recency and frequency. will lose them if not reactivated.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>At Risk</td>\n",
    "\t\t\t\t<td>Purchase often but a long time ago. Need to bring them back!</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Can't Lose Them</td>\n",
    "\t\t\t\t<td>Used to purchasde frequently but haven't returned for a long time.</td>\n",
    "\t\t\t</tr>\n",
    "\t\t\t<tr>\n",
    "\t\t\t\t<td>Hibernating</td>\n",
    "\t\t\t\t<td>Last purchase was long back and low number of orders. May be lost</td>\n",
    "\t\t\t</tr>\n",
    "        </tbody>\n",
    "\t</table>\n",
    "</body>\n",
    "\n",
    "- Champions: RFM score range - R: 4-5, F: 4-5, M: 4-5\n",
    "- Loyal customers: RFM score range - R: 3-5, F: 3-5, M: 3-5\n",
    "- Potential loyalist: RFM score range - R: 4-5, F: 2-3, M: 2-3\n",
    "- Recent customers: RFM score range - R: 4-5, F: 1-2, M: 1-2\n",
    "- Promising: RFM score range - R: 4-5, F: 1-2, M: 1-2\n",
    "- Needs attention: RFM score range - R: 3-5, F: 3-5, M: 3-5\n",
    "- About to sleep: RFM score range - R: 1-2, F: 1-2, M: 1-2\n",
    "- At risk: RFM score range - R: 2-5, F: 1-3, M: 1-3\n",
    "- Can't lose them: RFM score range - R: 1-3, F: 4-5, M: 4-5\n",
    "- Hibernating: RFM score range - R: 1-2, F: 1-2, M: 1-2\n",
    "\n",
    "**10.**  \n",
    "a) Visualize the RFM segments.  \n",
    "b) Conclude your findings of RFM analysis and suggest some strategies on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Lobraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import mlxtend\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime as dt\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"C:\\Users\\ASUS\\Downloads\\flights.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Flights Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://www.savethestudent.org/uploads/flights.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_excel(r\"Flight_Price_Test.xlsx\")\n",
    "\n",
    "# Check the top 5 records\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(r\"Flight_Price_Train.xlsx\")\n",
    "\n",
    "# Check the top 5 records\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perform Feature Engineering.\n",
    "**A) Perform basic exploration like checking for top 5 records, shape, statistical info, duplicates, Null values etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataset (number of rows and columns)\n",
    "\n",
    "print(\"Shape of the dataset:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical information about the numerical columns\n",
    "\n",
    "print(\"STATISTICAL INFORMATION ABOUT DATASET\")\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Statistical Summary\n",
    "\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns names\n",
    "\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for duplicates values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicated records in train data:\", df_train.duplicated().sum())\n",
    "print(\"Duplicated records in test data:\", df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a dataset with no customer data and only individual flights data.\n",
    "# We will drop duplicates from the training data to avoid overfitting on certain records.\n",
    "\n",
    "df_train.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "print(\"Duplicated records in train data:\", df_train.duplicated().sum())\n",
    "print(\"Duplicated records in test data:\", df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ckecking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only have 2 Null values, so we can drop them without a significant data loss\n",
    "\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Extract Date, Month, Year from Date of Journey column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extracting Date, Month, Year from Date of Journey column & Time(Minute and Hour) from Dep_Time column\n",
    "# Finally, we will drop 'Date_of_Journey', 'Dep_Time', 'Arrival_Time' columns\n",
    "\n",
    "df_train['Date_of_Journey'] = pd.to_datetime(df_train['Date_of_Journey'], format='%d/%m/%Y')\n",
    "\n",
    "df_train['Time'] = pd.to_datetime(df_train['Dep_Time'], format='%H:%M')\n",
    "df_train['Minute'] = df_train['Time'].dt.minute\n",
    "df_train['Hour'] = df_train['Time'].dt.hour\n",
    "df_train['Day'] = df_train['Date_of_Journey'].dt.day\n",
    "df_train['Month'] = df_train['Date_of_Journey'].dt.month\n",
    "df_train['Year'] = df_train['Date_of_Journey'].dt.year\n",
    "df_train.drop(columns=['Date_of_Journey', 'Dep_Time', 'Time', 'Arrival_Time'], axis=1, inplace=True)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert duration to minutes\n",
    "def duration_to_minutes(Duration):\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "\n",
    "    # Extract hours and minutes using regular expressions\n",
    "    hours_match = re.search(r'(\\d+)h', Duration)\n",
    "    minutes_match = re.search(r'(\\d+)m', Duration)\n",
    "\n",
    "    if hours_match is None:\n",
    "        hours = 0\n",
    "    else:\n",
    "        hours = int(hours_match.group(1))\n",
    "    if minutes_match is None:\n",
    "        minutes = 0\n",
    "    else:\n",
    "        minutes = int(hours_match.group(1))\n",
    "        \n",
    "\n",
    "    return hours * 60 + minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert duration to minutes\n",
    "def duration_to_minutes(Duration):\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "\n",
    "    # Extract hours and minutes using regular expressions\n",
    "    hours_match = re.search(r'(\\d+)h', Duration)\n",
    "    minutes_match = re.search(r'(\\d+)m', Duration)\n",
    "\n",
    "    if hours_match is None:\n",
    "        hours = 0\n",
    "    else:\n",
    "        hours = int(hours_match.group(1))\n",
    "\n",
    "    if minutes_match is None:\n",
    "        minutes = 0\n",
    "    else:\n",
    "        minutes = int(minutes_match.group(1))\n",
    "\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "# # Example usage\n",
    "# duration = \"2h 30m\"\n",
    "# result = duration_to_minutes(duration)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration column to minutes\n",
    "\n",
    "# Using function\n",
    "df_train['Duration'] = df_train['Duration'].apply(duration_to_minutes)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Additional_Info' column to string type\n",
    "df_train['Additional_Info'] = df_train['Additional_Info'].astype(str)\n",
    "\n",
    "# Replace 'No Info' with 'No info' (case-insensitive)\n",
    "df_train['Additional_Info'] = df_train['Additional_Info'].str.replace('No Info', 'No info', case=False)\n",
    "\n",
    "# Check the value counts\n",
    "df_train['Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date, Month, Year from Date of Journey column & Time(Minute and Hour) from Dep_Time column\n",
    "# Finally, we will drop 'Date_of_Journey', 'Dep_Time', 'Arrival_Time' columns\n",
    "\n",
    "df_test['Date_of_Journey'] = pd.to_datetime(df_test['Date_of_Journey'], format='%d/%m/%Y')\n",
    "\n",
    "df_test['Time'] = pd.to_datetime(df_test['Dep_Time'], format='%H:%M')\n",
    "df_test['Minute'] = df_test['Time'].dt.minute\n",
    "df_test['Hour'] = df_test['Time'].dt.hour\n",
    "df_test['Day'] = df_test['Date_of_Journey'].dt.day\n",
    "df_test['Month'] = df_test['Date_of_Journey'].dt.month\n",
    "df_test['Year'] = df_test['Date_of_Journey'].dt.year\n",
    "df_test.drop(columns=['Date_of_Journey', 'Dep_Time', 'Time', 'Arrival_Time'], axis=1, inplace=True)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting duration column to minutes\n",
    "\n",
    "# Using function\n",
    "df_test['Duration'] = df_test['Duration'].apply(duration_to_minutes)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to fix Additional_Info column in test data\n",
    "\n",
    "df_test['Additional_Info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform Exploratory Data Analysis (EDA) tasks\n",
    "**A) Which airline is most preferred airline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of flights per airline\n",
    "airline_counts = df_train['Airline'].value_counts()\n",
    "\n",
    "# Plot a bar chart to visualize the preferred airline\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=airline_counts.index, y=airline_counts.values)\n",
    "plt.title(\"Most Preferred Airline\")\n",
    "plt.xlabel(\"Airline\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=40)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The most preferred airline is : Jet Airways**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Find the majority of the flights take off from which source.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of flights taking off from each source\n",
    "source_counts = df_train['Source'].value_counts()\n",
    "\n",
    "# Plot a bar chart to visualize the majority of flight sources\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=source_counts.index, y=source_counts.values)\n",
    "plt.title(\"Majority of Flights by Source\")\n",
    "plt.xlabel(\"Source\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The majority of flights take off from : Delhi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Find maximum flights land in which destination.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of flights landing at each destination\n",
    "destination_counts = df_train['Destination'].value_counts()\n",
    "\n",
    "# Plot a bar chart to visualize the destination with the maximum flights\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=destination_counts.index, y=destination_counts.values)\n",
    "plt.title(\"Maximum Flights by Destination\")\n",
    "plt.xlabel(\"Destination\")\n",
    "plt.ylabel(\"Number of Flights\")\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum flights land in : Cochin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare independent features with Target feature to check the impact on price.\n",
    "**A) Which airline has the highest price.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to compare airlines and their average prices\n",
    "airline_prices = df_train.groupby('Airline')['Price'].mean().sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "airline_prices.plot(kind='bar')\n",
    "plt.title(\"Airline vs. Price\")\n",
    "plt.xlabel(\"Airline\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The airline with the highest average price is Jet Airways Business with a price of 79512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Check if the business class flights are high price or low and find only those flights which price is higher than 50k.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter flights with prices higher than 50k\n",
    "high_price_flights = df_train[df_train['Price'] > 50000]\n",
    "\n",
    "# Display the flights with high prices\n",
    "high_price_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['Additional_Info'] == \"Business class\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['Airline'] == \"Jet Airways Business\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['Airline'] == \"Vistara Premium economy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['Airline'] == \"Multiple carriers Premium economy\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Conclusion: \"Jet Airways Business\" having additional_info as \"Business class\" have the most expensive flights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform encoding for the required features according to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['Airline', 'Source', 'Destination', 'Total_Stops', 'Additional_Info']\n",
    "\n",
    "# Using function to label encode categorical columns of train data.\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    label_mapping_train = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    df_test[column] = label_encoder.transform(df_test[column])\n",
    "    label_mapping_test = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "    \n",
    "    print(f'Feature: {column}\\n')\n",
    "    print('Mapping_Train:', label_mapping_train,\"\\n\")\n",
    "    print('Mapping_Test:', label_mapping_test, \"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use One-Hot Encoder for \"Route\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming you have train and test DataFrames\n",
    "train_data = df_train['Route'].values.reshape(-1, 1)\n",
    "test_data = df_test['Route'].values.reshape(-1, 1)\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the train data\n",
    "train_encoded = encoder.fit_transform(train_data)\n",
    "\n",
    "# Transform the test data using the same encoder\n",
    "test_encoded = encoder.transform(test_data)\n",
    "\n",
    "# Convert the encoded sparse matrices to arrays if needed\n",
    "train_encoded_array = train_encoded.toarray()\n",
    "test_encoded_array = test_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for encoded train and test data\n",
    "train_encoded_df = pd.DataFrame(train_encoded_array, columns=encoder.get_feature_names_out(['Route']))\n",
    "test_encoded_df = pd.DataFrame(test_encoded_array, columns=encoder.get_feature_names_out(['Route']))\n",
    "\n",
    "# Reseting index to avoid generation of NaN values\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the encoded data with the original DataFrames\n",
    "df_train = pd.concat([df_train, train_encoded_df], axis=1)\n",
    "df_test = pd.concat([df_test, test_encoded_df], axis=1)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we will drop the 'Route' column from train and test datasets\n",
    "\n",
    "df_train.drop('Route', axis=1, inplace=True)\n",
    "df_test.drop('Route', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.shape)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_1 = df_train\n",
    "test_df_1 = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating into features variables and target variable.\n",
    "\n",
    "X = df_train.drop(columns=['Price'])  # Features\n",
    "Y = df_train['Price']  # Target variable\n",
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P has been defined using testing data\n",
    "\n",
    "P = test_df_1\n",
    "print(\"Shape of testing dataset:\", P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build multiple model by using different algorithm such as Linear Regression, Decision Tree, and Random Forest etc. and check the performance of your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "**Building a Random Forest Classifier model to check feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Splitting the data into test and train data sets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
    "\n",
    "# Creating a model object.\n",
    "model_DecisionTree = RandomForestClassifier(n_estimators=100, random_state=10)\n",
    "\n",
    "# Fitting the data into our model and predicting the values.\n",
    "model_DecisionTree.fit(X_train, Y_train)\n",
    "Y_pred = model_DecisionTree.predict(X_test)\n",
    "\n",
    "# Building a DataFrame to view the feature importance and sort for better visualization.\n",
    "feature_imp = pd.DataFrame()\n",
    "feature_imp[\"Feature\"] = train_df_1.drop([\"Price\"], axis=1).columns\n",
    "feature_imp[\"Importance\"] = model_DecisionTree.feature_importances_\n",
    "\n",
    "# Sorting the DataFrame by importance\n",
    "feature_imp_sorted = feature_imp.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "# Applying styling to hide the index\n",
    "styled_feature_imp = feature_imp_sorted.style.set_table_styles([{'selector': 'tr:hover','props': [('background-color', '')]}])\n",
    "\n",
    "# Displaying the styled DataFrame\n",
    "styled_feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining multiple Regression models\n",
    "\n",
    "models = [\n",
    "#   LinearRegression(), our data is high demesional so we refrain from using linear regression.\n",
    "    KNeighborsRegressor(),\n",
    "    DecisionTreeRegressor(random_state=10),\n",
    "    RandomForestRegressor(n_estimators=100, random_state=10),\n",
    "    ExtraTreesRegressor(n_estimators=100, random_state=10),\n",
    "    GradientBoostingRegressor(n_estimators=100, random_state=10)  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adj_r2(r2, n, k):\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model in models:\n",
    "    model.fit(X_train, Y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    adj_r2 = calculate_adj_r2(r2, len(Y_test), 1)  # Adjusted R-squared for one feature\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"Model: {type(model).__name__}\")\n",
    "    print(f'R-squared: {r2:.4f}')\n",
    "    print(f'Adjusted R-squared: {adj_r2:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(df_train.Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestRegressor is the best model for the given data with Rsquared = 0.9334. which means 93% of variablity in price is explained by the given features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 :  RFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recency, frequency, monetary value is a marketing analysis tool used to identify a company's or an organization's best customers by using certain measures. The RFM model is based on three quantitative factors:\n",
    "\n",
    "**Recency**: How recently a customer has made a purchase\n",
    "\n",
    "**Frequency**: How often a customer makes a purchase\n",
    "\n",
    "**Monetary** Value: How much money a customer spends on purchases\n",
    "\n",
    "RFM analysis numerically ranks a customer in each of these three categories, generally on a scale of 1 to 5 (the higher the number, the better the result). The \"best\" customer would receive a top score in every category.\n",
    "\n",
    "![](https://d35fo82fjcw0y8.cloudfront.net/2018/03/01013508/Incontent_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r\"C:RFM data.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing RFM Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['InvoiceDate'] = pd.to_datetime(df2['InvoiceDate'])\n",
    "\n",
    "print(\"Min date: {} \\nMax date: {}\".format(df2.InvoiceDate.min(), df2.InvoiceDate.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_day = df2['InvoiceDate'].max() + pd.Timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(last_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table = df2.groupby(\"CustomerID\").agg({\"InvoiceDate\": lambda x: (last_day - x.max()).days,\n",
    "                                          \"InvoiceNo\": \"nunique\",\n",
    "                                          \"TotalPrice\": \"sum\"})\n",
    "\n",
    "rfm_table.rename(columns = {\"InvoiceDate\": \"Recency\",\n",
    "                            \"InvoiceNo\": \"Frequency\",\n",
    "                            \"TotalPrice\": \"Monetary\"}, inplace = True)\n",
    "\n",
    "rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_labels = range(5, 0, -1)\n",
    "fm_labels = range(1, 6)\n",
    "\n",
    "rfm_table[\"R\"] = pd.qcut(rfm_table[\"Recency\"], 5, labels = r_labels)\n",
    "rfm_table[\"F\"] = pd.qcut(rfm_table[\"Frequency\"].rank(method = 'first'), 5, labels = fm_labels)\n",
    "rfm_table[\"M\"] = pd.qcut(rfm_table[\"Monetary\"], 5, labels = fm_labels)\n",
    "\n",
    "rfm_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table[\"RFM_Segment\"] = rfm_table[\"R\"].astype(str) + rfm_table[\"F\"].astype(str) + rfm_table[\"M\"].astype(str)\n",
    "rfm_table[\"RFM_Score\"] = rfm_table[[\"R\", \"F\", \"M\"]].sum(axis = 1)\n",
    "\n",
    "rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Champions**: Bought recently, buy often and spend the most\n",
    "\n",
    "**Loyal customers**: Buy on a regular basis. Responsive to promotions.\n",
    "\n",
    "**Potential loyalist**: Recent customers with average frequency.\n",
    "\n",
    "**Recent customers**: Bought most recently, but not often.\n",
    "\n",
    "**Promising**: Recent shoppers, but haven’t spent much.\n",
    "\n",
    "**Needs attention**: Above average recency, frequency and monetary values. May not have bought very recently though.\n",
    "\n",
    "**About to sleep**: Below average recency and frequency. Will lose them if not reactivated.\n",
    "\n",
    "**At risk**: Some time since they’ve purchased. Need to bring them back!\n",
    "\n",
    "**Can’t lose them**: Used to purchase frequently but haven’t returned for a long time.\n",
    "\n",
    "**Hibernating**: Last purchase was long back and low number of orders. May be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segt_map = {\n",
    "    r'[1-2][1-2]': 'Hibernating',\n",
    "    r'[1-2][3-4]': 'At-Risk',\n",
    "    r'[1-2]5': 'Cannot lose them',\n",
    "    r'3[1-2]': 'About To Sleep',\n",
    "    r'33': 'Need Attention',\n",
    "    r'[3-4][4-5]': 'Loyal Customers',\n",
    "    r'41': 'Promising',\n",
    "    r'51': 'New Customers',\n",
    "    r'[4-5][2-3]': 'Potential Loyalists',\n",
    "    r'5[4-5]': 'Champions'\n",
    "}\n",
    "rfm_table['Segment'] = rfm_table['R'].astype(str) + rfm_table['F'].astype(str)\n",
    "rfm_table['Segment'] = rfm_table['Segment'].replace(segt_map, regex=True)\n",
    "rfm_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing RFM Grid\n",
    "\n",
    "I wrote these codes for visualizing above RFM grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_coordinates = {\"Champions\": [3, 5, 0.8, 1],\n",
    "                   \"Loyal Customers\": [3, 5, 0.4, 0.8],\n",
    "                   \"Cannot lose them\": [4, 5, 0, 0.4],\n",
    "                   \"At-Risk\": [2, 4, 0, 0.4],\n",
    "                   \"Hibernating\": [0, 2, 0, 0.4],\n",
    "                   \"About To Sleep\": [0, 2, 0.4, 0.6],\n",
    "                   \"Promising\": [0, 1, 0.6, 0.8],\n",
    "                   \"New Customers\": [0, 1, 0.8, 1],\n",
    "                   \"Potential Loyalists\": [1, 3, 0.6, 1],\n",
    "                   \"Need Attention\": [2, 3, 0.4, 0.6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (17, 10))\n",
    "\n",
    "ax.set_xlim([0, 5])\n",
    "ax.set_ylim([0, 5])\n",
    "\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "palette = [\"#282828\", \"#04621B\", \"#971194\", \"#F1480F\",  \"#4C00FF\",\n",
    "           \"#FF007B\", \"#9736FF\", \"#8992F3\", \"#B29800\", \"#80004C\"]\n",
    "\n",
    "for key, color in zip(rfm_coordinates.keys(), palette[:10]):\n",
    "\n",
    "    coordinates = rfm_coordinates[key]\n",
    "    ymin, ymax, xmin, xmax = coordinates[0], coordinates[1], coordinates[2], coordinates[3]\n",
    "\n",
    "    ax.axhspan(ymin = ymin, ymax = ymax, xmin = xmin, xmax = xmax, facecolor = color)\n",
    "\n",
    "    users = rfm_table[rfm_table.Segment == key].shape[0]\n",
    "    users_percentage = (rfm_table[rfm_table.Segment == key].shape[0] / rfm_table.shape[0]) * 100\n",
    "    avg_monetary = rfm_table[rfm_table.Segment == key][\"Monetary\"].mean()\n",
    "\n",
    "    user_txt = \"\\n\\nTotal Users: \" + str(users) + \"(\" +  str(round(users_percentage, 2)) + \"%)\"\n",
    "    monetary_txt = \"\\n\\n\\n\\nAverage Monetary: \" + str(round(avg_monetary, 2))\n",
    "\n",
    "    x = 5 * (xmin + xmax) / 2\n",
    "    y = (ymin + ymax) / 2\n",
    "\n",
    "    plt.text(x = x, y = y, s = key, ha = \"center\", va = \"center\", fontsize = 18, color = \"white\", fontweight = \"bold\")\n",
    "    plt.text(x = x, y = y, s = user_txt, ha = \"center\", va = \"center\", fontsize = 14, color = \"white\")\n",
    "    plt.text(x = x, y = y, s = monetary_txt, ha = \"center\", va = \"center\", fontsize = 14, color = \"white\")\n",
    "\n",
    "    ax.set_xlabel(\"Recency Score\")\n",
    "    ax.set_ylabel(\"Frequency Score\")\n",
    "\n",
    "sns.despine(left = True, bottom = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing RFM Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_table2 = rfm_table.reset_index()\n",
    "\n",
    "rfm_monetary_size = rfm_table2.groupby(\"Segment\").agg({\"Monetary\": \"mean\",\n",
    "                                                       \"CustomerID\": \"nunique\"})\n",
    "\n",
    "rfm_monetary_size.rename(columns = {\"Monetary\": \"MeanMonetary\", \"CustomerID\": \"CustomerCount\"}, inplace = True)\n",
    "rfm_monetary_size = rfm_monetary_size.sort_values(\"MeanMonetary\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.facecolor\"] = \"#A2A2A2\"\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (16, 10), facecolor = \"#A2A2A2\")\n",
    "\n",
    "sns.barplot(x = rfm_monetary_size.MeanMonetary, y = rfm_monetary_size.index, ax = ax, color = \"#101820\")\n",
    "ax2 = ax.twiny()\n",
    "sns.lineplot(x = rfm_monetary_size.CustomerCount, y = rfm_monetary_size.index, ax = ax2, marker = \"o\", linewidth = 0,\n",
    "             color = \"#F1480F\", markeredgecolor = \"#F1480F\")\n",
    "\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "for y, x in list(enumerate(rfm_monetary_size.CustomerCount)):\n",
    "    ax2.text(x + 10, y + 0.05, str(x) + \" Customer\", color = \"white\", fontweight = \"normal\")\n",
    "\n",
    "plt.title(\"RFM Segments Details\")\n",
    "sns.despine(left = True, right = True, bottom = True, top = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary_per_segment = (rfm_table2.groupby(\"Segment\")[\"Monetary\"].sum() /\\\n",
    "                        rfm_table2.groupby(\"Segment\")[\"Monetary\"].sum().sum()).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 10), facecolor = \"#A2A2A2\")\n",
    "\n",
    "wedges, texts = ax.pie(monetary_per_segment.values, wedgeprops=dict(width=0.5), startangle=-40, normalize=False, colors = palette)\n",
    "\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "kw = dict(arrowprops=dict(arrowstyle=\"-\"),\n",
    "          bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "for i, p in enumerate(wedges):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "    ax.annotate(monetary_per_segment.index[i] + \" \" + str(round(monetary_per_segment[i] * 100, 2)) + \"%\", xy=(x, y),\n",
    "                xytext=(1.35*np.sign(x), 1.4*y),horizontalalignment=horizontalalignment, **kw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47.5% of total revenue comes from \"Champions\" segment, and 28% of total revenue comes from \"Loyal Customers\" segment. These two segments have 75% of company's total revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
